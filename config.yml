wanted_format:
  n_time_periods: 26
  joint_setup_cost: 5949
  minor_setup_ratio: 1.2
  should_include_safety_stock: True
  service level: [0.95, 0.95, 0.95, 0.95] 
deterministic_model:
  product_categories: { "erratic": 1, "smooth": 3 , "intermittent": 4, "lumpy": 3 }
  n_time_periods: 13
  joint_setup_cost: 5949
  minor_setup_ratio: 1.2
  service_level: 0.95
  should_include_safety_stock: True
rl_model:
  n_products: 6
  n_time_periods: 52
  joint_setup_cost: 5000
  minor_setup_cost: [ 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0 ]
  holding_cost: [ 56.746590597921845, 27.808012422471087, 55.161044780561035, 57.072195025923534, 60.601173777486366, 17.492847296869314]
  shortage_cost: [ 140,  140,  140,  140,  140,  140 ]
  base_unit_cost: [ 71.5, 109.0, 8.49, 3.19, 65.55, 65.55 ]
  big_m: [ 100, 100, 100, 100, 100, 100 ]
  service_level: [ 0.95, 0.95, 0.95, 0.95 , 0.95 ,0.95 ]
  epsilon: [ 0.00001 ]
  shipping_cost: [ 500 ]
  lower_bound_free_shipping: [ 4500 ]
  should_include_safety_stock: True

main:
  should_analyse: False
  stationary_products: True
  generate_new_data: True
  seed: 0
simulation:
  forecasting_method: "holt_winter"
  verbose: False
  should_set_holding_cost_dynamically: True
  n_episodes: 2
  simulation_length: 52
  warm_up_length: 104
  should_perform_warm_up: True
  reset_length: 20


actor:
  layers: [128,128]                        # Example: [30,30]
  activation_functions: ["tanh","tanh","relu"]  # Example: ["relu","relu"]. Examples of activation functions are relu linear, sigmoid, tanh
  learning_rate: 0.0001
  output_activation_function: "softmax"
  optimizer: "Adam"                                 # Examples of optimizers are Adagrad, Stochastic Gradient Descent (SGD), RMSprop or Adam
  loss_function: "mse"
  batch_size: 128                                   # Normal values: 32, 64, 128
  epochs: 10000
  epsilon: 0.5
  epsilon_decay: 0.995
  epsilon_min_value: 0.05
  episodes_epsilon_one: 100
  stochastic: False

critic:
  layers: [128,128]                        # Example: [30,30]
  activation_functions: ["tanh","tanh","relu"]  # Example: ["relu","relu"]. Examples of activation functions are relu linear, sigmoid, tanh
  learning_rate: 0.0001
  output_activation_function: "relu"
  optimizer: "Adam"                                 # Examples of optimizers are Adagrad, Stochastic Gradient Descent (SGD), RMSprop or Adam
  loss_function: "mse"
  batch_size: 128                                   # Normal values: 32, 64, 128
  epochs: 10000
  epsilon: 0.5
  epsilon_decay: 0.995
  epsilon_min_value: 0.05
  episodes_epsilon_one: 100
  stochastic: False
a2c:
  discount_rate: 0.99
  n_episodes: 1000
environment:
  n_periods_historical_data: 2
  rolling_window_forecast: 5
  # Note: maximum_order_quantity / n_action_classes must be an integer
  n_action_classes: 15
  maximum_order_quantity: 105
  order_quantity_per_discrete_class: 5
  should_include_individual_forecast: True
  should_include_total_forecast: False
rl:
  method: "ppo"
  generate_data: True

ppo:
  steps_per_epoch: 4000
  epochs: 3000
  gamma: 0.99
  clip_ratio: 0.2
  policy_learning_rate: 3e-4
  value_function_learning_rate: 1e-3
  train_policy_iterations: 80
  train_value_iterations: 80
  lam: 0.97
  target_kl: 0.01
  hidden_sizes: (64, 64)