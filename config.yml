deterministic_model:
  n_products: 6
  n_time_periods: 13
  joint_setup_cost: 500
  minor_setup_cost: [ 100.0, 100.0, 100.0, 100.0, 100.0, 100.0 ]
  holding_cost: [ 7.15,7.15,7.15,7.15, 7.15, 7.15 ]
  shortage_cost: [ 140,  140,  140,  140,  140,  140 ]
  base_unit_cost: [ 71.5, 109.0, 8.49, 3.19, 65.55, 65.55 ]
  big_m: [ 100, 100, 100, 100, 100, 100 ]
  service_level: [ 0.95, 0.95, 0.95, 0.95 , 0.95 ,0.95]
  epsilon: [ 0.00001 ]
  shipping_cost: [ 500 ]
  lower_bound_free_shipping: [ 4500 ]
  should_include_safety_stock: True
rl_model:
  n_products: 6
  n_time_periods: 13
  joint_setup_cost: 500
  minor_setup_cost: [ 100.0, 100.0, 100.0, 100.0, 100.0, 100.0 ]
  holding_cost: [ 7.15,7.15,7.15,7.15, 7.15, 7.15 ]
  shortage_cost: [ 140,  140,  140,  140,  140,  140 ]
  base_unit_cost: [ 71.5, 109.0, 8.49, 3.19, 65.55, 65.55 ]
  big_m: [ 100, 100, 100, 100, 100, 100 ]
  service_level: [ 0.95, 0.95, 0.95, 0.95 , 0.95 ,0.95 ]
  epsilon: [ 0.00001 ]
  shipping_cost: [ 500 ]
  lower_bound_free_shipping: [ 4500 ]
  should_include_safety_stock: True

main:
  should_analyse: False
  stationary_products: True
simulation:
  forecasting_method: "sarima"
  verbose: False
  should_set_holding_cost_dynamically: True

actor:
  layers: [128,128]                        # Example: [30,30]
  activation_functions: ["tanh","tanh","relu"]  # Example: ["relu","relu"]. Examples of activation functions are relu linear, sigmoid, tanh
  learning_rate: 0.0001
  output_activation_function: "softmax"
  optimizer: "Adam"                                 # Examples of optimizers are Adagrad, Stochastic Gradient Descent (SGD), RMSprop or Adam
  loss_function: "mse"
  batch_size: 128                                   # Normal values: 32, 64, 128
  epochs: 10000
  epsilon: 0.5
  epsilon_decay: 0.995
  epsilon_min_value: 0.05
  episodes_epsilon_one: 100
  stochastic: False

critic:
  layers: [128,128]                        # Example: [30,30]
  activation_functions: ["tanh","tanh","relu"]  # Example: ["relu","relu"]. Examples of activation functions are relu linear, sigmoid, tanh
  learning_rate: 0.0001
  output_activation_function: "relu"
  optimizer: "Adam"                                 # Examples of optimizers are Adagrad, Stochastic Gradient Descent (SGD), RMSprop or Adam
  loss_function: "mse"
  batch_size: 128                                   # Normal values: 32, 64, 128
  epochs: 10000
  epsilon: 0.5
  epsilon_decay: 0.995
  epsilon_min_value: 0.05
  episodes_epsilon_one: 100
  stochastic: False

a2c:
  discount_rate: 0.99
  n_episodes: 1000
environment:
  n_periods_historical_data: 0
  rolling_window_forecast: 5